"use strict";(globalThis.webpackChunktemp_docusaurus=globalThis.webpackChunktemp_docusaurus||[]).push([[5506],{8021(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"module4-vla/voice-to-action","title":"Voice-to-Action using OpenAI Whisper","description":"Enabling robots to understand and act upon spoken commands is a significant step towards more natural and intuitive human-robot interaction. OpenAI Whisper, a powerful speech-to-text model, provides an excellent foundation for translating human voice instructions into machine-readable commands that a robot can then execute.","source":"@site/docs/module4-vla/voice-to-action.md","sourceDirName":"module4-vla","slug":"/module4-vla/voice-to-action","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module4-vla/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/Muhammad-Rehan/Physical-AI-Humanoid-Robotics-Textbook/edit/main/docs/module4-vla/voice-to-action.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Path Planning for Bipedal Humanoids with Nav2","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module3-isaac/path-planning-nav2"},"next":{"title":"Cognitive Planning with LLMs","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/module4-vla/cognitive-planning-llm"}}');var t=i(4848),a=i(8453);const r={},s="Voice-to-Action using OpenAI Whisper",c={},l=[{value:"1.1 Introduction to Voice Commands in Robotics",id:"11-introduction-to-voice-commands-in-robotics",level:2},{value:"Motivation: Natural human-robot interaction",id:"motivation-natural-human-robot-interaction",level:3},{value:"Challenges: Speech recognition accuracy, noise, latency",id:"challenges-speech-recognition-accuracy-noise-latency",level:3},{value:"1.2 Overview of OpenAI Whisper",id:"12-overview-of-openai-whisper",level:2},{value:"What is Whisper? Large language model for speech-to-text",id:"what-is-whisper-large-language-model-for-speech-to-text",level:3},{value:"Key features: High accuracy, multilingual support, robust to noise",id:"key-features-high-accuracy-multilingual-support-robust-to-noise",level:3},{value:"Deployment options: API, local models",id:"deployment-options-api-local-models",level:3},{value:"1.3 Integrating Whisper for Robot Command Recognition",id:"13-integrating-whisper-for-robot-command-recognition",level:2},{value:"Setting up Whisper (API client or local environment)",id:"setting-up-whisper-api-client-or-local-environment",level:3},{value:"Capturing audio input from microphone",id:"capturing-audio-input-from-microphone",level:3},{value:"Transcribing speech to text commands",id:"transcribing-speech-to-text-commands",level:3},{value:"1.4 A Simple Python Example of using the Whisper API",id:"14-a-simple-python-example-of-using-the-whisper-api",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"voice-to-action-using-openai-whisper",children:"Voice-to-Action using OpenAI Whisper"})}),"\n",(0,t.jsx)(n.p,{children:"Enabling robots to understand and act upon spoken commands is a significant step towards more natural and intuitive human-robot interaction. OpenAI Whisper, a powerful speech-to-text model, provides an excellent foundation for translating human voice instructions into machine-readable commands that a robot can then execute."}),"\n",(0,t.jsx)(n.h2,{id:"11-introduction-to-voice-commands-in-robotics",children:"1.1 Introduction to Voice Commands in Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"motivation-natural-human-robot-interaction",children:"Motivation: Natural human-robot interaction"}),"\n",(0,t.jsx)(n.p,{children:"Traditional robot interfaces often rely on joysticks, programming languages, or graphical user interfaces. While effective for trained operators, these methods can be cumbersome and unnatural for casual users or in situations requiring quick, hands-free interaction. Voice commands offer a more intuitive and accessible way for humans to communicate with robots, mimicking natural human-human communication."}),"\n",(0,t.jsx)(n.p,{children:"Benefits include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accessibility:"})," Allows users without specialized training to interact with robots."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Hands-Free Operation:"})," Crucial in environments where a human's hands are occupied (e.g., surgical assistance, assembly tasks)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speed and Efficiency:"})," Can convey complex instructions faster than typing or clicking."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intuitive:"})," Aligns with human communication patterns, reducing cognitive load."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"challenges-speech-recognition-accuracy-noise-latency",children:"Challenges: Speech recognition accuracy, noise, latency"}),"\n",(0,t.jsx)(n.p,{children:"Implementing robust voice command systems in robotics comes with several challenges:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech Recognition Accuracy:"})," Accurately transcribing diverse human speech, including varying accents, speech rates, and vocabulary."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Noise Robustness:"})," Real-world environments are often noisy (e.g., robot motors, background conversations, factory sounds). The speech recognition system must be able to filter out noise and accurately capture the commands."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latency:"})," The delay between a spoken command and the robot's action needs to be minimal for a natural and responsive interaction. High latency can lead to frustration and potential safety issues."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Command Parsing and Understanding:"})," After transcription, the raw text needs to be understood semantically and translated into executable robot actions. This involves natural language understanding (NLU) to interpret intent, entities, and parameters."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety and Redundancy:"})," Voice commands alone might not be sufficient for safety-critical applications. Redundant safety mechanisms are often required."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"12-overview-of-openai-whisper",children:"1.2 Overview of OpenAI Whisper"}),"\n",(0,t.jsx)(n.p,{children:"OpenAI Whisper is a general-purpose speech recognition model developed by OpenAI. It was trained on a massive dataset of diverse audio and corresponding textual transcripts, enabling it to achieve high accuracy and robustness across various languages and conditions."}),"\n",(0,t.jsx)(n.h3,{id:"what-is-whisper-large-language-model-for-speech-to-text",children:"What is Whisper? Large language model for speech-to-text"}),"\n",(0,t.jsx)(n.p,{children:"Whisper is an encoder-decoder Transformer model that takes raw audio as input and outputs the corresponding text. Unlike previous speech recognition models that often struggled with varying languages, accents, and background noise, Whisper's training on a vast and diverse dataset makes it highly capable in these areas."}),"\n",(0,t.jsx)(n.h3,{id:"key-features-high-accuracy-multilingual-support-robust-to-noise",children:"Key features: High accuracy, multilingual support, robust to noise"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High Accuracy:"})," Whisper achieves state-of-the-art results on many speech recognition benchmarks, often outperforming previous models, especially on challenging audio."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multilingual Support:"})," Trained on multilingual data, it can transcribe and translate speech in many languages."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robust to Noise:"})," Its extensive training data, including noisy audio, makes it highly resilient to background noise and environmental distractions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speaker Diarization (limited):"})," While not its primary focus, Whisper can sometimes implicitly distinguish between speakers in a conversation."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Punctuation and Capitalization:"})," Generates text with good punctuation and capitalization, improving readability and downstream natural language processing."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"deployment-options-api-local-models",children:"Deployment options: API, local models"}),"\n",(0,t.jsx)(n.p,{children:"Whisper can be accessed in a few ways:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OpenAI API:"})," The easiest way to use Whisper, offering various models (tiny, base, small, medium, large) with different accuracy-to-speed trade-offs. It handles the underlying computational complexity."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Local Models:"})," OpenAI has open-sourced the Whisper models, allowing developers to run them locally on their own hardware (CPU or GPU). This is ideal for applications requiring offline functionality, custom integration, or specific privacy requirements. Several community-driven projects have also optimized Whisper for local deployment (e.g., ",(0,t.jsx)(n.code,{children:"whisper.cpp"}),", ",(0,t.jsx)(n.code,{children:"faster-whisper"}),")."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"13-integrating-whisper-for-robot-command-recognition",children:"1.3 Integrating Whisper for Robot Command Recognition"}),"\n",(0,t.jsx)(n.p,{children:"Integrating Whisper into a robot's control system involves capturing audio, transcribing it, and then processing the text to extract commands."}),"\n",(0,t.jsx)(n.h3,{id:"setting-up-whisper-api-client-or-local-environment",children:"Setting up Whisper (API client or local environment)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"API Client:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Install the OpenAI Python library: ",(0,t.jsx)(n.code,{children:"pip install openai"})]}),"\n",(0,t.jsxs)(n.li,{children:["Obtain an OpenAI API key and set it as an environment variable (",(0,t.jsx)(n.code,{children:"OPENAI_API_KEY"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:["Use the ",(0,t.jsx)(n.code,{children:"openai.Audio.transcribe"})," method."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Local Environment:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Install the ",(0,t.jsx)(n.code,{children:"whisper"})," Python package: ",(0,t.jsx)(n.code,{children:"pip install -U openai-whisper"})]}),"\n",(0,t.jsxs)(n.li,{children:["Choose a model size (",(0,t.jsx)(n.code,{children:"tiny"}),", ",(0,t.jsx)(n.code,{children:"base"}),", ",(0,t.jsx)(n.code,{children:"small"}),", ",(0,t.jsx)(n.code,{children:"medium"}),", ",(0,t.jsx)(n.code,{children:"large"}),") based on your accuracy and performance needs."]}),"\n",(0,t.jsxs)(n.li,{children:["Load the model: ",(0,t.jsx)(n.code,{children:'model = whisper.load_model("base")'})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"capturing-audio-input-from-microphone",children:"Capturing audio input from microphone"}),"\n",(0,t.jsxs)(n.p,{children:["For real-time voice commands, you need to capture audio from the robot's (or user's) microphone. Python libraries like ",(0,t.jsx)(n.code,{children:"sounddevice"})," or ",(0,t.jsx)(n.code,{children:"PyAudio"})," can be used for this."]}),"\n",(0,t.jsx)(n.p,{children:"Key considerations:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sampling Rate:"})," Match the sampling rate expected by Whisper (typically 16kHz)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Chunking:"})," Process audio in small chunks to maintain responsiveness."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice Activity Detection (VAD):"})," Optionally, use VAD to detect when a user is speaking and filter out silence, improving efficiency and reducing processing load."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"transcribing-speech-to-text-commands",children:"Transcribing speech to text commands"}),"\n",(0,t.jsx)(n.p,{children:"Once audio is captured, it's passed to Whisper for transcription."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import whisper\n\n# Load the desired Whisper model\n# model = whisper.load_model("base")\n\ndef transcribe_audio_file(audio_path):\n    """Transcribes an audio file using the local Whisper model."""\n    result = model.transcribe(audio_path)\n    return result["text"]\n\n# Example usage (requires an audio file)\n# text = transcribe_audio_file("my_command.wav")\n# print(f"Transcribed text: {text}")\n'})}),"\n",(0,t.jsx)(n.p,{children:"For real-time streaming, you would buffer audio chunks and periodically send them to Whisper for transcription, or use a streaming-optimized Whisper implementation."}),"\n",(0,t.jsx)(n.h2,{id:"14-a-simple-python-example-of-using-the-whisper-api",children:"1.4 A Simple Python Example of using the Whisper API"}),"\n",(0,t.jsx)(n.p,{children:"This example outlines how you might use Whisper (conceptual API or local) to process a voice command and rudimentary parsing."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import speech_recognition as sr\nimport openai # For OpenAI API, or local whisper if preferred\nimport time\n\n# --- Configuration ---\n# Use a local Whisper model\n# import whisper\n# model = whisper.load_model("base") \n\n# Or use OpenAI API\n# openai.api_key = "YOUR_OPENAI_API_KEY" # Replace with your actual API key or use environment variable\n\ndef get_audio_from_mic():\n    """Captures audio from the microphone until silence is detected."""\n    r = sr.Recognizer()\n    with sr.Microphone() as source:\n        print("Say something!")\n        # Adjust for ambient noise for better recognition\n        r.adjust_for_ambient_noise(source) \n        audio = r.listen(source, phrase_time_limit=5) # Listen for up to 5 seconds of speech\n    print("Audio captured. Processing...")\n    return audio\n\ndef transcribe_audio_whisper(audio_data):\n    """Transcribes audio data using OpenAI Whisper (API or local)."""\n    try:\n        # Save audio to a temporary WAV file (required by Whisper for file input)\n        with open("temp_command.wav", "wb") as f:\n            f.write(audio_data.get_wav_data())\n\n        # Option 1: Using local Whisper model\n        # transcribed_text = model.transcribe("temp_command.wav")["text"]\n        \n        # Option 2: Using OpenAI Whisper API\n        with open("temp_command.wav", "rb") as audio_file:\n            response = openai.audio.transcriptions.create(\n                model="whisper-1", \n                file=audio_file, \n                language="en" # Specify language for better accuracy\n            )\n        transcribed_text = response.text\n\n        return transcribed_text.strip().lower()\n    except Exception as e:\n        print(f"Could not transcribe audio: {e}")\n        return ""\n\ndef parse_robot_command(text):\n    """Simple parsing of transcribed text into a robot action and parameters."""\n    if "move forward" in text:\n        return {"action": "move", "direction": "forward", "distance": 1.0}\n    elif "move backward" in text:\n        return {"action": "move", "direction": "backward", "distance": 1.0}\n    elif "turn left" in text:\n        return {"action": "turn", "direction": "left", "angle": 90}\n    elif "turn right" in text:\n        return {"action": "turn", "direction": "right", "angle": 90}\n    elif "stop" in text:\n        return {"action": "stop"}\n    else:\n        return {"action": "unknown", "command": text}\n\ndef execute_robot_action(command_data):\n    """Placeholder for actually sending commands to a ROS 2 robot."""\n    if command_data["action"] == "move":\n        print(f"Robot command: Move {command_data[\'direction\']} by {command_data[\'distance\']} meters")\n        # Here you would typically publish a Twist message or call a ROS 2 action\n    elif command_data["action"] == "turn":\n        print(f"Robot command: Turn {command_data[\'direction\']} by {command_data[\'angle\']} degrees")\n        # Here you would typically publish a Twist message with angular velocity\n    elif command_data["action"] == "stop":\n        print("Robot command: Stop all motion")\n        # Here you would typically publish a Twist message with zero velocities\n    else:\n        print(f"Unknown command: {command_data[\'command\']}")\n\nif __name__ == "__main__":\n    while True:\n        audio_data = get_audio_from_mic()\n        if audio_data:\n            transcribed_text = transcribe_audio_whisper(audio_data)\n            print(f"Transcribed: {transcribed_text}")\n            command = parse_robot_command(transcribed_text)\n            execute_robot_action(command)\n        else:\n            print("No audio detected.")\n        time.sleep(1) # Wait a bit before listening again\n'})}),"\n",(0,t.jsxs)(n.p,{children:["This example shows the basic flow: capturing audio, transcribing it with Whisper, and then a rudimentary parsing function to extract actionable commands. In a real ",(0,t.jsx)(n.a,{href:"pathname:///docs/module1-ros2/introduction",children:"ROS 2"})," system, ",(0,t.jsx)(n.code,{children:"execute_robot_action"})," would involve publishing messages to topics (e.g., ",(0,t.jsx)(n.code,{children:"geometry_msgs/msg/Twist"})," for movement) or calling ",(0,t.jsx)(n.a,{href:"pathname:///docs/module1-ros2/introduction",children:"ROS 2"})," services/actions."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>s});var o=i(6540);const t={},a=o.createContext(t);function r(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);