<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module4-vla/cognitive-planning-llm" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Cognitive Planning with LLMs | PHYSICAL AI &amp; HUMANOID ROBOTICS COURSE</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://muhammad-rehan.github.io/Physical-AI-Humanoid-Robotics-Textbook/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://muhammad-rehan.github.io/Physical-AI-Humanoid-Robotics-Textbook/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://muhammad-rehan.github.io/Physical-AI-Humanoid-Robotics-Textbook/docs/module4-vla/cognitive-planning-llm"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Cognitive Planning with LLMs | PHYSICAL AI &amp; HUMANOID ROBOTICS COURSE"><meta data-rh="true" name="description" content="Large Language Models (LLMs) are transforming various domains, and robotics is no exception. Their ability to understand and generate human-like text, coupled with their vast encapsulated knowledge, makes them powerful tools for cognitive planning, enabling robots to interpret high-level commands and generate sequences of actions to achieve complex goals."><meta data-rh="true" property="og:description" content="Large Language Models (LLMs) are transforming various domains, and robotics is no exception. Their ability to understand and generate human-like text, coupled with their vast encapsulated knowledge, makes them powerful tools for cognitive planning, enabling robots to interpret high-level commands and generate sequences of actions to achieve complex goals."><link data-rh="true" rel="icon" href="/Physical-AI-Humanoid-Robotics-Textbook/img/favicon.jpg"><link data-rh="true" rel="canonical" href="https://muhammad-rehan.github.io/Physical-AI-Humanoid-Robotics-Textbook/docs/module4-vla/cognitive-planning-llm"><link data-rh="true" rel="alternate" href="https://muhammad-rehan.github.io/Physical-AI-Humanoid-Robotics-Textbook/docs/module4-vla/cognitive-planning-llm" hreflang="en"><link data-rh="true" rel="alternate" href="https://muhammad-rehan.github.io/Physical-AI-Humanoid-Robotics-Textbook/docs/module4-vla/cognitive-planning-llm" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Cognitive Planning with LLMs","item":"https://muhammad-rehan.github.io/Physical-AI-Humanoid-Robotics-Textbook/docs/module4-vla/cognitive-planning-llm"}]}</script><link rel="alternate" type="application/rss+xml" href="/Physical-AI-Humanoid-Robotics-Textbook/blog/rss.xml" title="PHYSICAL AI &amp; HUMANOID ROBOTICS COURSE RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/Physical-AI-Humanoid-Robotics-Textbook/blog/atom.xml" title="PHYSICAL AI &amp; HUMANOID ROBOTICS COURSE Atom Feed"><link rel="stylesheet" href="/Physical-AI-Humanoid-Robotics-Textbook/assets/css/styles.92f412a0.css">
<script src="/Physical-AI-Humanoid-Robotics-Textbook/assets/js/runtime~main.f4d6c742.js" defer="defer"></script>
<script src="/Physical-AI-Humanoid-Robotics-Textbook/assets/js/main.c1a303f5.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/Physical-AI-Humanoid-Robotics-Textbook/img/logo.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Physical-AI-Humanoid-Robotics-Textbook/"><div class="navbar__logo"><img src="/Physical-AI-Humanoid-Robotics-Textbook/img/logo.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Physical-AI-Humanoid-Robotics-Textbook/img/logo.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/intro/introduction">Book</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="translation-wrapper"><div class=""><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/intro/introduction"><span title="Introduction" class="categoryLinkLabel_W154">Introduction</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/getting-started/ros2-setup"><span title="Getting Started" class="categoryLinkLabel_W154">Getting Started</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/module1-ros2/introduction"><span title="Module 1: ROS 2" class="categoryLinkLabel_W154">Module 1: ROS 2</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/module2-gazebo/physics-simulation"><span title="Module 2: Simulation" class="categoryLinkLabel_W154">Module 2: Simulation</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/module3-isaac/isaac-sim"><span title="Module 3: Isaac &amp; VSLAM" class="categoryLinkLabel_W154">Module 3: Isaac &amp; VSLAM</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/module4-vla/voice-to-action"><span title="Module 4: VLA" class="categoryLinkLabel_W154">Module 4: VLA</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/module4-vla/voice-to-action"><span title="Voice-to-Action using OpenAI Whisper" class="linkLabel_WmDU">Voice-to-Action using OpenAI Whisper</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/module4-vla/cognitive-planning-llm"><span title="Cognitive Planning with LLMs" class="linkLabel_WmDU">Cognitive Planning with LLMs</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/module4-vla/capstone-project"><span title="Capstone Project" class="categoryLinkLabel_W154">Capstone Project</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/resources/"><span title="Resources" class="categoryLinkLabel_W154">Resources</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Physical-AI-Humanoid-Robotics-Textbook/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module 4: VLA</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Cognitive Planning with LLMs</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Cognitive Planning with LLMs</h1></header>
<p>Large Language Models (LLMs) are transforming various domains, and robotics is no exception. Their ability to understand and generate human-like text, coupled with their vast encapsulated knowledge, makes them powerful tools for cognitive planning, enabling robots to interpret high-level commands and generate sequences of actions to achieve complex goals.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="21-introduction-to-large-language-models-llms-in-robotics">2.1 Introduction to Large Language Models (LLMs) in Robotics<a href="#21-introduction-to-large-language-models-llms-in-robotics" class="hash-link" aria-label="Direct link to 2.1 Introduction to Large Language Models (LLMs) in Robotics" title="Direct link to 2.1 Introduction to Large Language Models (LLMs) in Robotics" translate="no">​</a></h2>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="motivation-high-level-reasoning-task-decomposition-common-sense-knowledge">Motivation: High-level reasoning, task decomposition, common-sense knowledge<a href="#motivation-high-level-reasoning-task-decomposition-common-sense-knowledge" class="hash-link" aria-label="Direct link to Motivation: High-level reasoning, task decomposition, common-sense knowledge" title="Direct link to Motivation: High-level reasoning, task decomposition, common-sense knowledge" translate="no">​</a></h3>
<p>Traditional robotics heavily relies on explicitly programmed rules and finite state machines for task execution. This approach becomes brittle and difficult to scale for open-ended tasks in unstructured environments. LLMs offer a paradigm shift:</p>
<ul>
<li class=""><strong>High-level Reasoning:</strong> LLMs can process abstract human instructions (e.g., &quot;make coffee,&quot; &quot;clean the table&quot;) and translate them into logical steps, leveraging their extensive pre-trained knowledge base.</li>
<li class=""><strong>Task Decomposition:</strong> Complex tasks can be broken down into a series of simpler, executable sub-tasks, a process that is often challenging to hardcode. LLMs can dynamically decompose tasks based on context.</li>
<li class=""><strong>Common-Sense Knowledge:</strong> LLMs implicitly encode a vast amount of common-sense knowledge about the world, which is crucial for handling unexpected situations or making inferences that are not explicitly programmed. This allows robots to move beyond rigid scripts.</li>
<li class=""><strong>Adaptability:</strong> LLMs can be prompted to adapt their planning based on new information, environmental changes, or user feedback, leading to more flexible robot behavior.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="challenges-grounding-llm-outputs-in-physical-reality-safety-computational-cost">Challenges: Grounding LLM outputs in physical reality, safety, computational cost<a href="#challenges-grounding-llm-outputs-in-physical-reality-safety-computational-cost" class="hash-link" aria-label="Direct link to Challenges: Grounding LLM outputs in physical reality, safety, computational cost" title="Direct link to Challenges: Grounding LLM outputs in physical reality, safety, computational cost" translate="no">​</a></h3>
<p>While promising, integrating LLMs into robotics presents significant challenges:</p>
<ul>
<li class=""><strong>Grounding LLM Outputs in Physical Reality:</strong> LLMs operate in a linguistic space, not a physical one. Their outputs (textual plans) need to be &quot;grounded&quot; in the robot&#x27;s physical capabilities and the real-world environment. This means translating abstract instructions into specific motor commands, navigation goals, or manipulation sequences. The robot&#x27;s perception of the world must align with the LLM&#x27;s understanding.</li>
<li class=""><strong>Safety:</strong> LLMs can sometimes generate unexpected, incorrect, or even unsafe instructions. Ensuring that an LLM-driven robot operates safely requires robust validation layers, human oversight, and fail-safe mechanisms.</li>
<li class=""><strong>Computational Cost:</strong> Running large LLMs, especially in real-time on edge devices, can be computationally expensive and may require significant hardware resources. Optimization and efficient inference techniques are critical.</li>
<li class=""><strong>Ambiguity and Hallucination:</strong> LLMs can hallucinate information or produce ambiguous instructions that are difficult for a robot to interpret reliably. Careful prompt engineering and feedback loops are necessary.</li>
<li class=""><strong>Limited Real-time Feedback:</strong> Standard LLMs do not inherently operate in real-time feedback loops with the physical world. Mechanisms are needed to allow the robot&#x27;s current state and perception to influence the LLM&#x27;s ongoing planning.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="22-llms-for-task-decomposition-and-high-level-planning">2.2 LLMs for Task Decomposition and High-Level Planning<a href="#22-llms-for-task-decomposition-and-high-level-planning" class="hash-link" aria-label="Direct link to 2.2 LLMs for Task Decomposition and High-Level Planning" title="Direct link to 2.2 LLMs for Task Decomposition and High-Level Planning" translate="no">​</a></h2>
<p>One of the most powerful applications of LLMs in robotics is their ability to decompose high-level, abstract goals into a sequence of actionable steps.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="using-llms-to-break-down-complex-natural-language-goals-into-a-sequence-of-executable-robot-actions">Using LLMs to break down complex natural language goals into a sequence of executable robot actions<a href="#using-llms-to-break-down-complex-natural-language-goals-into-a-sequence-of-executable-robot-actions" class="hash-link" aria-label="Direct link to Using LLMs to break down complex natural language goals into a sequence of executable robot actions" title="Direct link to Using LLMs to break down complex natural language goals into a sequence of executable robot actions" translate="no">​</a></h3>
<p>Consider the goal: &quot;Make me breakfast.&quot; This seemingly simple instruction involves many sub-tasks: go to the kitchen, identify breakfast items, retrieve ingredients, operate appliances, prepare food, serve it, and clean up. An LLM can help structure this process.</p>
<p>The general approach involves:</p>
<ol>
<li class=""><strong>Receive High-Level Goal:</strong> The user provides a natural language instruction (e.g., &quot;Clean the room&quot;).</li>
<li class=""><strong>Prompt LLM for Plan:</strong> The LLM is prompted with the goal, along with information about the robot&#x27;s capabilities (available tools, actions) and the current state of the environment.</li>
<li class=""><strong>Generate Plan Steps:</strong> The LLM outputs a textual plan, breaking down the complex goal into a sequence of simpler steps. These steps might be abstract (e.g., &quot;Go to the table&quot;) or more specific (e.g., &quot;Pick up the red cup&quot;).</li>
<li class=""><strong>Execute Steps Sequentially:</strong> Each step of the plan is then translated into robot-executable actions.</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="example-clean-the-room---go-to-table-pick-up-cup-put-cup-in-sink">Example: &quot;Clean the room&quot; -&gt; &quot;Go to table&quot;, &quot;Pick up cup&quot;, &quot;Put cup in sink&quot;<a href="#example-clean-the-room---go-to-table-pick-up-cup-put-cup-in-sink" class="hash-link" aria-label="Direct link to Example: &quot;Clean the room&quot; -&gt; &quot;Go to table&quot;, &quot;Pick up cup&quot;, &quot;Put cup in sink&quot;" title="Direct link to Example: &quot;Clean the room&quot; -&gt; &quot;Go to table&quot;, &quot;Pick up cup&quot;, &quot;Put cup in sink&quot;" translate="no">​</a></h3>
<p>Let&#x27;s imagine a robot is tasked with &quot;Clean the room.&quot; A well-engineered prompt could elicit a plan similar to this:</p>
<p><strong>Prompt to LLM:</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">You are a helpful robot assistant. Your goal is to generate a step-by-step plan for a robot to perform a task.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">The robot can perform the following actions:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- navigate_to(location): Move to a specified location.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- pick_up(object): Pick up an object.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- place_object(object, location): Place an object at a specified location.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- detect_objects(): Scan the environment for objects.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Current environment state:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- The room contains a table, a couch, and a sink.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- On the table: a red cup, a book.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- On the couch: a blue pillow.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Task: Clean the room.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Generate a numbered list of actions the robot should take to clean the room.</span><br></span></code></pre></div></div>
<p><strong>LLM Response (Example):</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">1. navigate_to(table)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. pick_up(red cup)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. navigate_to(sink)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">4. place_object(red cup, sink)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">5. navigate_to(table)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">6. pick_up(book)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">7. navigate_to(shelf) # Assuming &#x27;shelf&#x27; is a valid location for books</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">8. place_object(book, shelf)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">9. navigate_to(couch)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">10. pick_up(blue pillow)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">11. place_object(blue pillow, couch_original_position) # Or a designated pillow area</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">12. navigate_to(charging_station) # Go to idle or charge</span><br></span></code></pre></div></div>
<p>This plan then needs to be parsed and executed by the robot&#x27;s control system.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="23-prompt-engineering-for-robot-control">2.3 Prompt Engineering for Robot Control<a href="#23-prompt-engineering-for-robot-control" class="hash-link" aria-label="Direct link to 2.3 Prompt Engineering for Robot Control" title="Direct link to 2.3 Prompt Engineering for Robot Control" translate="no">​</a></h2>
<p>The quality of an LLM&#x27;s plan heavily depends on the quality of the prompt. Effective prompt engineering is key to guiding the LLM to generate safe, feasible, and relevant robot actions.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="designing-effective-prompts-to-guide-llms-towards-generating-safe-and-feasible-plans">Designing effective prompts to guide LLMs towards generating safe and feasible plans<a href="#designing-effective-prompts-to-guide-llms-towards-generating-safe-and-feasible-plans" class="hash-link" aria-label="Direct link to Designing effective prompts to guide LLMs towards generating safe and feasible plans" title="Direct link to Designing effective prompts to guide LLMs towards generating safe and feasible plans" translate="no">​</a></h3>
<p>Key strategies for prompt engineering:</p>
<ul>
<li class=""><strong>Define Robot Capabilities:</strong> Clearly state the actions the robot can perform, including their arguments and expected outcomes.</li>
<li class=""><strong>Provide Contextual Information:</strong> Include relevant details about the robot&#x27;s current state, the environment, and the goal. The more specific and up-to-date this information, the better.</li>
<li class=""><strong>Specify Output Format:</strong> Instruct the LLM to provide the plan in a structured, easily parsable format (e.g., a numbered list of function calls).</li>
<li class=""><strong>Include Constraints and Safety Guidelines:</strong> Explicitly tell the LLM about safety constraints (e.g., &quot;Do not approach humans too closely,&quot; &quot;Avoid dropping objects&quot;).</li>
<li class=""><strong>Few-Shot Learning:</strong> Provide examples of desired task decompositions to guide the LLM&#x27;s behavior.</li>
<li class=""><strong>Iterative Refinement:</strong> If the initial plan is not satisfactory, provide feedback to the LLM and ask it to refine the plan.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="providing-a-prompt-engineering-example-showing-how-to-translate-clean-the-room-into-a-sequence-of-actions">Providing a prompt engineering example showing how to translate &quot;Clean the room&quot; into a sequence of actions<a href="#providing-a-prompt-engineering-example-showing-how-to-translate-clean-the-room-into-a-sequence-of-actions" class="hash-link" aria-label="Direct link to Providing a prompt engineering example showing how to translate &quot;Clean the room&quot; into a sequence of actions" title="Direct link to Providing a prompt engineering example showing how to translate &quot;Clean the room&quot; into a sequence of actions" translate="no">​</a></h3>
<p>(This has been demonstrated in the previous section&#x27;s example)</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="24-grounding-llm-plans-into-robot-executable-actions">2.4 Grounding LLM Plans into Robot Executable Actions<a href="#24-grounding-llm-plans-into-robot-executable-actions" class="hash-link" aria-label="Direct link to 2.4 Grounding LLM Plans into Robot Executable Actions" title="Direct link to 2.4 Grounding LLM Plans into Robot Executable Actions" translate="no">​</a></h2>
<p>Once an LLM generates a textual plan, the robot needs to convert these abstract steps into concrete, low-level commands that its hardware can execute. This is the &quot;grounding&quot; problem.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="mapping-llm-generated-high-level-actions-to-specific-robot-apis-ros-2-services-actions-topics">Mapping LLM-generated high-level actions to specific robot APIs (ROS 2 services, actions, topics)<a href="#mapping-llm-generated-high-level-actions-to-specific-robot-apis-ros-2-services-actions-topics" class="hash-link" aria-label="Direct link to Mapping LLM-generated high-level actions to specific robot APIs (ROS 2 services, actions, topics)" title="Direct link to Mapping LLM-generated high-level actions to specific robot APIs (ROS 2 services, actions, topics)" translate="no">​</a></h3>
<p>Each action in the LLM&#x27;s plan (e.g., <code>navigate_to(location)</code>, <code>pick_up(object)</code>) must correspond to a specific robot API:</p>
<ul>
<li class=""><strong><code>navigate_to(location)</code>:</strong> Might map to a <a href="/Physical-AI-Humanoid-Robotics-Textbook/docs/module1-ros2/introduction" target="_blank" rel="noopener noreferrer" class="">ROS 2</a> Action Goal for the <a href="/Physical-AI-Humanoid-Robotics-Textbook/docs/module3-isaac/vslam-navigation" target="_blank" rel="noopener noreferrer" class="">Nav2</a> stack (<code>NavigateToPose</code>). The <code>location</code> would be translated into a <code>geometry_msgs/PoseStamped</code>.</li>
<li class=""><strong><code>pick_up(object)</code>:</strong> Could map to a <a href="/Physical-AI-Humanoid-Robotics-Textbook/docs/module1-ros2/introduction" target="_blank" rel="noopener noreferrer" class="">ROS 2</a> Service call to a manipulation controller (e.g., a custom service like <code>gripper_control/srv/PickUpObject</code>). The <code>object</code> would need to be localized using perception and its pose passed as a parameter.</li>
<li class=""><strong><code>place_object(object, location)</code>:</strong> Similar to <code>pick_up</code>, this would invoke a manipulation service or action, with <code>object</code> and <code>location</code> translated into target poses.</li>
<li class=""><strong><code>detect_objects()</code>:</strong> Might trigger a perception system (e.g., an Isaac ROS DNN node) via a ROS 2 Service call, which then publishes detected objects to a topic.</li>
</ul>
<p>This mapping layer acts as an interpreter, translating the LLM&#x27;s linguistic instructions into the robot&#x27;s operational language.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="handling-ambiguities-and-failure-cases-asking-for-clarification-replanning">Handling ambiguities and failure cases: asking for clarification, replanning<a href="#handling-ambiguities-and-failure-cases-asking-for-clarification-replanning" class="hash-link" aria-label="Direct link to Handling ambiguities and failure cases: asking for clarification, replanning" title="Direct link to Handling ambiguities and failure cases: asking for clarification, replanning" translate="no">​</a></h3>
<p>Real-world robot operation is prone to uncertainties. The grounding layer must handle:</p>
<ul>
<li class=""><strong>Ambiguity:</strong> If the LLM&#x27;s instruction is vague (e.g., &quot;get the thing&quot;), the robot might need to ask for clarification from the user or use its perception to infer intent.</li>
<li class=""><strong>Failure Cases:</strong> If a planned action fails (e.g., <code>pick_up(object)</code> fails because the object is too heavy or not reachable), the robot needs mechanisms to:<!-- -->
<ul>
<li class=""><strong>Report Failure:</strong> Inform the user.</li>
<li class=""><strong>Replanning:</strong> Ask the LLM to generate an alternative plan, potentially with updated environmental information or new constraints.</li>
<li class=""><strong>Error Recovery:</strong> Execute predefined error recovery routines.</li>
</ul>
</li>
</ul>
<p>This iterative feedback loop, where the robot&#x27;s real-world experiences inform and refine the LLM&#x27;s cognitive planning, is crucial for developing truly intelligent and robust robotic systems.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/Muhammad-Rehan/Physical-AI-Humanoid-Robotics-Textbook/edit/main/docs/module4-vla/cognitive-planning-llm.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/module4-vla/voice-to-action"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Voice-to-Action using OpenAI Whisper</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/module4-vla/capstone-project"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Capstone Project: The Autonomous Humanoid</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-introduction-to-large-language-models-llms-in-robotics" class="table-of-contents__link toc-highlight">2.1 Introduction to Large Language Models (LLMs) in Robotics</a><ul><li><a href="#motivation-high-level-reasoning-task-decomposition-common-sense-knowledge" class="table-of-contents__link toc-highlight">Motivation: High-level reasoning, task decomposition, common-sense knowledge</a></li><li><a href="#challenges-grounding-llm-outputs-in-physical-reality-safety-computational-cost" class="table-of-contents__link toc-highlight">Challenges: Grounding LLM outputs in physical reality, safety, computational cost</a></li></ul></li><li><a href="#22-llms-for-task-decomposition-and-high-level-planning" class="table-of-contents__link toc-highlight">2.2 LLMs for Task Decomposition and High-Level Planning</a><ul><li><a href="#using-llms-to-break-down-complex-natural-language-goals-into-a-sequence-of-executable-robot-actions" class="table-of-contents__link toc-highlight">Using LLMs to break down complex natural language goals into a sequence of executable robot actions</a></li><li><a href="#example-clean-the-room---go-to-table-pick-up-cup-put-cup-in-sink" class="table-of-contents__link toc-highlight">Example: &quot;Clean the room&quot; -&gt; &quot;Go to table&quot;, &quot;Pick up cup&quot;, &quot;Put cup in sink&quot;</a></li></ul></li><li><a href="#23-prompt-engineering-for-robot-control" class="table-of-contents__link toc-highlight">2.3 Prompt Engineering for Robot Control</a><ul><li><a href="#designing-effective-prompts-to-guide-llms-towards-generating-safe-and-feasible-plans" class="table-of-contents__link toc-highlight">Designing effective prompts to guide LLMs towards generating safe and feasible plans</a></li><li><a href="#providing-a-prompt-engineering-example-showing-how-to-translate-clean-the-room-into-a-sequence-of-actions" class="table-of-contents__link toc-highlight">Providing a prompt engineering example showing how to translate &quot;Clean the room&quot; into a sequence of actions</a></li></ul></li><li><a href="#24-grounding-llm-plans-into-robot-executable-actions" class="table-of-contents__link toc-highlight">2.4 Grounding LLM Plans into Robot Executable Actions</a><ul><li><a href="#mapping-llm-generated-high-level-actions-to-specific-robot-apis-ros-2-services-actions-topics" class="table-of-contents__link toc-highlight">Mapping LLM-generated high-level actions to specific robot APIs (ROS 2 services, actions, topics)</a></li><li><a href="#handling-ambiguities-and-failure-cases-asking-for-clarification-replanning" class="table-of-contents__link toc-highlight">Handling ambiguities and failure cases: asking for clarification, replanning</a></li></ul></li></ul></div></div></div></div></main></div></div></div></div><div style="position:fixed;top:18px;right:130px;z-index:1000;display:flex;align-items:center"></div><div class="rag-chatbot"><button class="chatbot-toggle" aria-label="Open chat">?</button></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Textbook/docs/intro/introduction">Textbook</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Textbook/">Stack Overflow</a></li><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Textbook/">Discord</a></li><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Textbook/">X</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Physical-AI-Humanoid-Robotics-Textbook/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/Muhammad-Rehan/Physical-AI-Humanoid-Robotics-Textbook" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 AI Robotics Textbook. Made By Muhammad Rehan.</div></div></div></footer></div>
</body>
</html>